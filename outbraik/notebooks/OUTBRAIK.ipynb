{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Cross validation</a></span></li><li><span><a href=\"#Create-Tensorflow-DNN-model\" data-toc-modified-id=\"Create-Tensorflow-DNN-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create Tensorflow DNN model</a></span></li><li><span><a href=\"#Bayesian-optimisation\" data-toc-modified-id=\"Bayesian-optimisation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bayesian optimisation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# Tensorflow:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Scikit-Optimise\n",
    "from skopt import gp_minimize, dump\n",
    "from skopt.space import Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataframe, n_splits, random_state):\n",
    "    \"\"\"Scikit-Learn KFold implementation for pandas DataFrame.\"\"\"\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    kfolds = []\n",
    "    global offset_col_name\n",
    "\n",
    "    for train, validate in kf.split(dataframe):\n",
    "        training = dataset.iloc[train]\n",
    "        train_labels = training[offset_col_name]\n",
    "        train_set = training.drop(offset_col_name, axis=1)\n",
    "\n",
    "        validating = dataset.iloc[validate]\n",
    "        validate_labels = validating[offset_col_name]\n",
    "        validate_set = validating.drop(offset_col_name, axis=1)\n",
    "\n",
    "        kfolds.append(\n",
    "            [[train_set, validate_set],\n",
    "             [train_labels, validate_labels]]\n",
    "        )\n",
    "\n",
    "    with open('./outbraik/data/07_model_output/kfolds.json', \"wb\") as file:\n",
    "        pickle.dump(kfolds, file)\n",
    "\n",
    "    logging.info('Pickled kfolds nested list to JSON.')\n",
    "    return kfolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tensorflow DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_dense_layers_base, num_dense_nodes_base,\n",
    "                 num_dense_layers_end, num_dense_nodes_end,\n",
    "                 activation, adam_b1, adam_b2, adam_eps):\n",
    "    \"\"\"Returns \"\"\"\n",
    "\n",
    "    # Craete linear stack of layers.\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Define input layer.\n",
    "    model.add(keras.layers.Dense(\n",
    "        num_input_nodes,  # N.umber of nodes\n",
    "        input_shape=(num_input_nodes,)  # Tuple specifying data input dimensions only needed in first layer.\n",
    "             ))\n",
    "\n",
    "    # Define n number of hidden layers (base, i.e. first layers).\n",
    "    for i in range(num_dense_layers_base):\n",
    "        model.add(keras.layers.Dense(\n",
    "            num_dense_nodes_base,\n",
    "            activation=activation\n",
    "        ))\n",
    "\n",
    "    # Define n number of hidden layers (end, i.e. last layers).\n",
    "    for i in range(num_dense_layers_end):\n",
    "        model.add(keras.layers.Dense(\n",
    "            num_dense_nodes_end,\n",
    "            activation=activation\n",
    "        ))\n",
    "\n",
    "    # Define output layer.\n",
    "    model.add(keras.layers.Dense(1, activation=keras.activations.linear))\n",
    "\n",
    "    # Define dam optimiser.\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        lr=0.0001,  # Learning rate\n",
    "        beta_1=adam_b1,  # Exponential decay rate for the first moment estimates.\n",
    "        beta_2=adam_b2,  # Exponential decay rate for the second-moment estimates.\n",
    "        epsilon=adam_eps  # Prevent any division by zero.\n",
    "    )\n",
    "\n",
    "    # Compile model.\n",
    "    model.compile(\n",
    "        loss='mae',  # Loss function\n",
    "        optimizer=optimizer,  # Optimisaion function defined above.\n",
    "        metrics=['mae']  # Metric to be recorded.\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fold, fold_num):\n",
    "    \"\"\"\n",
    "    1. Unpack training data.\n",
    "    2. Define hyper-perameter ranges.\n",
    "    3. Define early stopping perameters.\n",
    "    4. Optimise hyper-perameters and save best model.\n",
    "    5. Save mae per call to CSV.\n",
    "    \"\"\"\n",
    "    logging.info('Training fold {}.'.format(str(fold_num)))\n",
    "    \n",
    "    # Retrieve data sets and convert to numpy array.\n",
    "    train_X = fold[0][0].values\n",
    "    validate_X = fold[0][1].values\n",
    "    train_y = fold[1][0].values\n",
    "    validate_y = fold[1][1].values\n",
    "\n",
    "    # Define hyper-perameters.\n",
    "    # Layers\n",
    "    dim_num_dense_layers_base = Integer(low=1, high=2, name='num_dense_layers_base')\n",
    "    dim_num_dense_nodes_base = Categorical(categories=list(np.linspace(5, 261, 10, dtype=int)),\n",
    "                                           name='num_dense_nodes_base')\n",
    "    dim_num_dense_layers_end = Integer(low=1, high=2, name='num_dense_layers_end')\n",
    "    dim_num_dense_nodes_end = Categorical(categories=list(np.linspace(5, 261, 10, dtype=int)),\n",
    "                                          name='num_dense_nodes_end')\n",
    "\n",
    "    # Optimiser\n",
    "    dim_adam_b1 = Categorical(categories=list(np.linspace(0.8, 0.99, 11)), name='adam_b1')\n",
    "    dim_adam_b2 = Categorical(categories=list(np.linspace(0.8, 0.99, 11)), name='adam_b2')\n",
    "    dim_adam_eps = Categorical(categories=list(np.linspace(0.0001, 0.5, 11)), name='adam_eps')\n",
    "\n",
    "    dimensions = [dim_num_dense_layers_base, dim_num_dense_nodes_base,\n",
    "                  dim_num_dense_layers_end, dim_num_dense_nodes_end,\n",
    "                  dim_adam_b1, dim_adam_b2, dim_adam_eps]\n",
    "\n",
    "    # Set early stopping variable to prevent overfitting.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',  # Monitor validation loss\n",
    "        mode='min',  # Monitoring loss\n",
    "        patience=20,  # Large patience for small batch size\n",
    "        verbose=0)  # Do not output to terminal\n",
    "\n",
    "    # Start hyper-perameter optimisation.\n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def fitness(num_dense_layers_base, num_dense_nodes_base,\n",
    "                num_dense_layers_end, num_dense_nodes_end,\n",
    "                adam_b1, adam_b2, adam_eps):\n",
    "\n",
    "        # Create the neural network with these hyper-parameters.\n",
    "        model = create_model(num_dense_layers_base=num_dense_layers_base,\n",
    "                             num_dense_nodes_base=num_dense_nodes_base,\n",
    "                             num_dense_layers_end=num_dense_layers_end,\n",
    "                             num_dense_nodes_end=num_dense_nodes_end,\n",
    "                             activation=tf.keras.activations.relu,\n",
    "                             adam_b1=adam_b1, adam_b2=adam_b2, adam_eps=adam_eps)\n",
    "\n",
    "        history = model.fit(train_X, train_y, # Training data\n",
    "                            epochs=epochs,  # Number of forward and backward runs.\n",
    "                            validation_data=(validate_X, validate_y),  # Validation data\n",
    "                            verbose=1,\n",
    "                            callbacks=[early_stopping],  # Prevent overfitting.\n",
    "                            batch_size=30)  # Increase efficiency\n",
    "\n",
    "        # If the regressor accuracy of the saved model is improved...\n",
    "        global best_mae\n",
    "        if mae < best_mae:\n",
    "            # Save the new model to harddisk.\n",
    "            model.save(output_dr + 'ddGhydr_' + model_type + '_fold_' + str(fold_num) + '_model.h5')\n",
    "            # Update the regressor accuracy.\n",
    "            best_mae = mae\n",
    "\n",
    "        # Delete the Keras model with these hyper-parameters from memory.\n",
    "        del model\n",
    "\n",
    "        # Clear the Keras session, otherwise it will keep adding new\n",
    "        # models to the same TensorFlow graph each time we create\n",
    "        # a model with a different set of hyper-parameters.\n",
    "        K.clear_session()\n",
    "\n",
    "        # Reset best MAE.\n",
    "        best_mae = np.inf\n",
    "\n",
    "        return mae\n",
    "\n",
    "    # A place for optimiser to start looking.\n",
    "    default_parameters = [2, 261, 1, 61, 0.857, 0.933, 0.20006]\n",
    "\n",
    "    search_result = gp_minimize(func=fitness,\n",
    "                                dimensions=dimensions,\n",
    "                                acq_func='EI',  # Expected Improvement\n",
    "                                n_calls=n_calls,\n",
    "                                x0=default_parameters)\n",
    "\n",
    "    # Save skopt object.\n",
    "    dump(search_result,\n",
    "         './outbraik/data/07_model_output/fold_' + str(fold_num) +  '_gp_minimize_result.pickle',\n",
    "         store_objective=False)\n",
    "    logging.info('Pickled fold {} Scikit-Optimise object.'.format(fold_num))\n",
    "\n",
    "    logging.info('Fold {} final parameters: {}.'.format(str(fold_num), search_result.x))\n",
    "    return search_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
